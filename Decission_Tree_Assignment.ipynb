{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decission Tree**"
      ],
      "metadata": {
        "id": "RyJ-nCOy-YGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is a Decision Tree, and how does it work in the context of classification?**"
      ],
      "metadata": {
        "id": "VMmqLvxM-wVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:** A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks, but it is most commonly explained in the context of classification.\n",
        "How it works in Classification\n",
        "\n",
        "**Root Node Selection**\n",
        "\n",
        "The tree starts with a root node that contains the entire dataset.\n",
        "The algorithm chooses the best feature to split the dataset.\n",
        "Criteria for \"best\" are usually:\n",
        "Gini Impurity,\n",
        "Entropy / Information Gain (from Information Theory)\n",
        "\n",
        "**Splitting the Data**\n",
        "\n",
        "The chosen feature is used to partition the dataset into subsets.\n",
        "\n",
        "Example: If the feature is \"Age > 30,\" data gets divided into two branches.\n",
        "\n",
        "**Recursive Partitioning**\n",
        "\n",
        "The algorithm repeats the process for each subset (choosing the best feature at that node).\n",
        "\n",
        "This continues until:\n",
        "All data points in a node belong to the same class, OR\n",
        "Maximum depth / stopping criteria are reached.\n",
        "\n",
        "**Leaf Node Assignment**\n",
        "\n",
        "At the end of each path, the node is assigned a class label (majority class of samples in that node)."
      ],
      "metadata": {
        "id": "2DYFPJDR_I5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?**\n"
      ],
      "metadata": {
        "id": "_3aFzg1aABVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:**\n",
        "\n",
        "**Gini Impurity**\n",
        "\n",
        "It measures the probability of incorrectly classifying a randomly chosen element.\n",
        "Value 0 means pure (all samples in one class). Higher values indicate more impurity.\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "It measures the disorder or uncertainty in the node.\n",
        "0 = pure, 1 = maximum impurity for binary classes.\n",
        "\n",
        "**Impact on Decision Tree Splits**\n",
        "\n",
        "At each node, the algorithm chooses the feature that gives the largest reduction in impurity.\n",
        "\n",
        "Gini is faster and often preferred in CART (Classification and Regression Trees).\n",
        "\n",
        "Entropy (via Information Gain) is more sensitive to class distribution.\n",
        "\n",
        "Both aim to create child nodes that are as pure as possible, improving classification accuracy."
      ],
      "metadata": {
        "id": "EzR5zk6wAgBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n"
      ],
      "metadata": {
        "id": "x4TWpN7xBK2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:**\n",
        "\n",
        "**Pre-Pruning (Early Stopping):**\n",
        "\n",
        "The tree growth is stopped early, before it becomes too complex.\n",
        "\n",
        "Stopping conditions may include: maximum depth, minimum samples per node, or minimum information gain.\n",
        "\n",
        "**Advantage:** Saves computation and prevents the tree from overfitting at the start.\n",
        "\n",
        "**Post-Pruning (Pruning after Full Growth):**\n",
        "\n",
        "The tree is grown fully and then pruned back by removing branches that do not improve accuracy.\n",
        "\n",
        "Methods: cost complexity pruning, reduced error pruning.\n",
        "\n",
        "**Advantage:** Produces simpler, more general trees while still considering all possible splits first."
      ],
      "metadata": {
        "id": "m7zVjDCgB0mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "FEJfxvptCN2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**:\n",
        "**Information Gain**  measures the reduction in impurity (or uncertainty) achieved after splitting a dataset on a feature\n",
        "\n",
        "**Importance for Splits:**\n",
        "\n",
        "A higher IG means the feature provides more “information” about the target class.\n",
        "\n",
        "Decision Trees choose the feature with the highest Information Gain at each node.\n",
        "\n",
        "This ensures that the tree splits data into purer subsets, improving classification accuracy."
      ],
      "metadata": {
        "id": "FMU_L6xhCh0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n"
      ],
      "metadata": {
        "id": "S7Ac3h9cDCm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**:\n",
        "**Applications of Decision Trees**\n",
        "\n",
        "**Medical Diagnosis** – predicting diseases from patient symptoms.\n",
        "\n",
        "**Finance** – credit scoring, loan approval, fraud detection.\n",
        "\n",
        "**Marketing** – customer segmentation, predicting customer churn.\n",
        "\n",
        "**Operations** – risk analysis, decision support systems.\n",
        "\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "Easy to understand and interpret (like flowcharts).\n",
        "\n",
        "Handles both numerical and categorical data.\n",
        "\n",
        "Requires little data preprocessing (no scaling/normalization needed).\n",
        "\n",
        "Can capture non-linear relationships.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "Prone to overfitting if not pruned.\n",
        "\n",
        "Small changes in data can lead to a completely different tree (high variance).\n",
        "\n",
        "Biased towards features with more categories.\n",
        "\n",
        "Less accurate compared to ensemble methods (e.g., Random Forests)."
      ],
      "metadata": {
        "id": "8TpnuSZnDPOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6: Write a Python program to:1 Load the Iris Dataset 2 Train a Decision Tree Classifier using the Gini criterion 3 Print the model’s accuracy and feature importances**\n",
        " (● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).)"
      ],
      "metadata": {
        "id": "MXYsK-HgDr1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#  Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "#  Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=52\n",
        ")\n",
        "\n",
        "#  Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=52)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#  Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYLIg-T2ET2u",
        "outputId": "449b0d83-81cc-4774-8061-89ad606173fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0191\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.9374\n",
            "petal width (cm): 0.0435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:● Load the Iris Dataset● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**\n"
      ],
      "metadata": {
        "id": "lBBNTLigGH_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Decision Tree Classifier - Comparing max_depth vs Full Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=69\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=69)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=69)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# 5. Print accuracies\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_score(y_test, y_pred_limited))\n",
        "print(\"Accuracy with full tree:\", accuracy_score(y_test, y_pred_full))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qggmb-WaGmgn",
        "outputId": "33b5006d-7ee1-44bb-da01-31ce05db43c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 0.9777777777777777\n",
            "Accuracy with full tree: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: Write a Python program to:● Load the Boston Housing Dataset● Train a Decision Tree Regressor● Print the Mean Squared Error (MSE) and feature importances**\n",
        "(Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV))"
      ],
      "metadata": {
        "id": "rayYYcrbcsAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=69)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = pd.Series(regressor.feature_importances_, index=X.columns)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGT8K99YdLJd",
        "outputId": "ea2b8c37-9567-493f-de37-63e55f72cff7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 9.574\n",
            "\n",
            "Feature Importances:\n",
            "RM         0.590075\n",
            "LSTAT      0.191007\n",
            "CRIM       0.071858\n",
            "DIS        0.064809\n",
            "TAX        0.026275\n",
            "AGE        0.015267\n",
            "INDUS      0.011848\n",
            "B          0.010606\n",
            "PTRATIO    0.007085\n",
            "NOX        0.006802\n",
            "RAD        0.002061\n",
            "ZN         0.001994\n",
            "CHAS       0.000312\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Write a Python program to:● Load the Iris Dataset● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV● Print the best parameters and the resulting model accuracy**\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n"
      ],
      "metadata": {
        "id": "tj73ZyzVe2wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import  libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict on test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmBOwEzxfIUu",
        "outputId": "0d8e52a4-38bd-477d-e3ad-4934e1f0f92f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.**\n"
      ],
      "metadata": {
        "id": "_l4GOC7ei7T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: Handle Missing Values:-**\n",
        "Ill check if there is any missing value .\n",
        "\n",
        "with: **df.isnull().sum()** function.\n",
        "\n",
        "if any missing values:-\n",
        "\n",
        "for:\n",
        "\n",
        "**Numerical features:** Use mean, median, or KNN imputation.\n",
        "\n",
        "**Categorical features:** Use mode  or a new category like “Unknown\n",
        "\n",
        "\n",
        " **2: Encode Categorical Features**\n",
        "\n",
        " Decision Trees in sklearn can handle ordinal categories as integers, but one-hot encoding is safer for non-ordinal categorical variables.\n",
        "\n",
        " Merge encoded categorical features with numerical features.\n",
        "\n",
        "Alternatively, use ColumnTransformer to handle numerical and categorical preprocessing in one pipeline.\n",
        "\n",
        "\n",
        "**3: Train a Decision Tree Model**\n",
        "\n",
        "Split data into training and test sets:\n",
        "\n",
        "Train a Decision Tree Classifier:\n",
        "\n",
        "\n",
        " **4: Tune Hyperparameters**\n",
        "\n",
        "Goal: Avoid overfitting and improve performance.\n",
        "\n",
        "Important hyperparameters for Decision Tree:\n",
        "\n",
        "**max_depth:** Maximum depth of the tree.\n",
        "\n",
        "**min_samples_split:** Minimum samples required to split a node.\n",
        "\n",
        "**min_samples_leaf:** Minimum samples required at a leaf node.\n",
        "\n",
        "criterion: “gini” or “entropy” for split quality.\n",
        "\n",
        "\n",
        "**5: Business Value in Real-World Healthcare**\n",
        "\n",
        "**Early Disease Detection:** Predicting disease risk early allows timely intervention.\n",
        "\n",
        "**Resource Optimization:** Helps hospitals prioritize high-risk patients for tests or treatments.\n",
        "\n",
        "**Personalized Care:** Tailor patient care based on predicted risk.\n",
        "\n",
        "**Cost Reduction:** Reduce unnecessary tests for low-risk patients while focusing on high-risk ones.\n",
        "\n",
        "**Data-Driven Decisions:** Provides actionable insights for clinicians and management.\n",
        "\n",
        "**Example:** If the model predicts high risk of diabetes in a patient, the hospital can schedule further tests and preventive care earlier, improving patient outcomes and reducing treatment costs later."
      ],
      "metadata": {
        "id": "_cWFJXc7jrhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C9w1U_N3mst5"
      }
    }
  ]
}